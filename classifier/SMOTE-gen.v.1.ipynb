{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "731e688a-071f-4f2f-9835-9c0f7fa1b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import requests\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Generator, Tuple\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"smote_process.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2080a2-1eba-4d35-aeab-e83a93123480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuración inicial\n",
    "LOCAL_IMAGE_PATH = './repo_dataset'\n",
    "TARGET_SIZE = (224, 224)\n",
    "TARGET_SIZE_CHANNEL = (224, 224, 3)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Columnas de clases\n",
    "LABEL_COLUMNS = ['direccion', 'fachada', 'envio', 'etiqueta', 'planilla']\n",
    "\n",
    "\n",
    "#cargar csv y dividir en dev set y test set\n",
    "# TRAIN\n",
    "#CSV_PATH = './mobilnet-multi-label-con-planilla-all-data.csv'\n",
    "#CSV_PATH_DEV = './mobilnet-multi-label-train-80-planilla.csv'\n",
    "#CSV_PATH_TEST = './mobilnet-multi-label-devtest-20-planilla.csv'\n",
    "\n",
    "# DEV\n",
    "CSV_PATH = './mobilnet-multi-label-devtest-20-planilla.csv'\n",
    "CSV_PATH_DEV = './mobilnet-multi-label-dev-50-planilla.csv'\n",
    "CSV_PATH_TEST = './mobilnet-multi-label-test-50-planilla.csv'\n",
    "\n",
    "CSV_TRAIN = CSV_PATH_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e46f5854-a4f1-4213-8d82-738496d43425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar las imagenes\n",
    "def prepare_image(row, local_image_path, label_columns, target_size):\n",
    "    # Preparar las etiquetas\n",
    "    labels = row[label_columns].values.astype(int)\n",
    " \n",
    "    try:\n",
    "        # Cargar desde archivo local\n",
    "        img_path = os.path.join(local_image_path, row['filename'])\n",
    "        if os.path.exists(img_path):\n",
    "            image = Image.open(img_path)\n",
    "        elif pd.notna(row['urlAbsoluta']):    \n",
    "             urlAbsoluta = row['urlAbsoluta']\n",
    "             if 'http' in urlAbsoluta:\n",
    "                 # Descargar la imagen desde la URL\n",
    "                 response = requests.get(row['urlAbsoluta'], stream=True, timeout=10)\n",
    "                 if response.status_code == 200:\n",
    "                     image = Image.open(BytesIO(response.content))\n",
    "                     #guardar local para el siguiente ciclo de entrenamiento/prueba\n",
    "                     image.save(img_path)\n",
    "             elif os.path.exists(urlAbsoluta):\n",
    "                 image = Image.open(urlAbsoluta)\n",
    "             else:\n",
    "                 raise Exception(f'Error cargando {urlAbsoluta}, archivo no encontrado')\n",
    "    \n",
    "        # Convertir a RGB (en caso de que la imagen esté en otro formato, como RGBA)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Redimensionar la imagen\n",
    "        image = image.resize(target_size)  # Redimensionar a 224x224 para MobileNetV3\n",
    "        \n",
    "        # Convertir a un array de numpy y normalizar\n",
    "        image = np.array(image) / 255.0  # Normalizar\n",
    "        \n",
    "        return image, np.array(labels)\n",
    "    except BaseException as e:\n",
    "        print(f'Error en: {img_path}, Excepción: {str(e)}')\n",
    "        return None\n",
    "\n",
    "def print_class_distribution_from_csv(csv_path, label_columns):\n",
    "    \"\"\"\n",
    "    Imprime la distribución de clases leyendo desde un archivo CSV\n",
    "    \n",
    "    Parámetros:\n",
    "    csv_path: str - Ruta al archivo CSV\n",
    "    label_columns: list - Lista de nombres de las columnas de etiquetas\n",
    "    \"\"\"\n",
    "    # Leer solo las columnas necesarias del CSV\n",
    "    df = pd.read_csv(csv_path, usecols=label_columns)\n",
    "    total_samples = len(df)\n",
    "    frecuencias = [0] * len(label_columns)\n",
    "    \n",
    "    print(f\"Dataset preparado con {total_samples} imágenes\")\n",
    "    print(f\"Distribución de clases:\")\n",
    "    \n",
    "    for idx, col in enumerate(label_columns):\n",
    "        positive_samples = df[col].sum()\n",
    "        percentage = (positive_samples / total_samples) * 100\n",
    "        print(f\"{col}: {percentage:.2f}% ({int(positive_samples)}/{total_samples})\")\n",
    "        frecuencias[idx] = positive_samples\n",
    "    return total_samples, frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ce8370a-83bc-460a-b873-26836e458fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(csv_path, local_image_path, label_columns, target_size, batch_size=32):\n",
    "    \"\"\"Generador de lotes de ejemplo\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    for start in range(0, total_samples, batch_size):\n",
    "        batch = df.iloc[start:start+batch_size]\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            result = prepare_image(row, local_image_path, label_columns, target_size)\n",
    "            if result is not None:\n",
    "                img_array, img_labels = result\n",
    "                X_batch.append(img_array)\n",
    "                y_batch.append(img_labels)\n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        yield np.array(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c204c2b7-6b91-477b-bf06-ae115fde1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelSMOTE:\n",
    "    def __init__(self, target_samples=500, k_neighbors=5, output_dir='synthetic', batch_size=100, img_shape=TARGET_SIZE_CHANNEL):\n",
    "        self.target_samples = target_samples\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.csv_path = self.csv_path = os.path.join(output_dir, 'metadata.csv')\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self._init_csv()\n",
    "        \n",
    "        # Estado del proceso\n",
    "        self.class_stats = {\n",
    "            'direccion': {'original': 0, 'synthetic': 0},\n",
    "            'fachada': {'original': 0, 'synthetic': 0},\n",
    "            'envio': {'original': 0, 'synthetic': 0},\n",
    "            'etiqueta': {'original': 0, 'synthetic': 0},\n",
    "            'planilla': {'original': 0, 'synthetic': 0}\n",
    "        }\n",
    "        self.existing_hashes = set()\n",
    "        self._load_existing_hashes()\n",
    "\n",
    "    def _init_csv(self) -> None:\n",
    "        \"\"\"Inicializa el archivo CSV de metadatos\"\"\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            pd.DataFrame(columns=['filename', 'urlAbsoluta', 'direccion', \n",
    "                                'fachada', 'envio', 'etiqueta', 'planilla']).to_csv(self.csv_path, index=False)\n",
    "\n",
    "    def _load_existing_hashes(self) -> None:\n",
    "        \"\"\"Carga hashes existentes de ejecuciones previas\"\"\"\n",
    "        hash_file = os.path.join(self.output_dir, 'image_hashes.txt')\n",
    "        try:\n",
    "            if os.path.exists(hash_file):\n",
    "                with open(hash_file, 'r') as f:\n",
    "                    self.existing_hashes = set(f.read().splitlines())\n",
    "                logger.info(f\"Loaded {len(self.existing_hashes)} existing hashes\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading hashes: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _validate_batch(self, X_batch: np.ndarray, y_batch: np.ndarray) -> None:\n",
    "        \"\"\"Valida el formato de los datos de entrada\"\"\"\n",
    "        # Validar etiquetas binarias\n",
    "        if not np.array_equal(y_batch, y_batch.astype(bool)):\n",
    "            raise ValueError(\"Las etiquetas deben ser valores binarios (0 o 1)\")\n",
    "        \n",
    "        # Validar rango de imágenes\n",
    "        if (X_batch.dtype != np.float32 and X_batch.dtype != np.float64) or np.min(X_batch) < 0 or np.max(X_batch) > 1:\n",
    "            raise ValueError(\"Las imágenes deben estar en formato float32 o float64 y normalizadas [0, 1]\")\n",
    "            \n",
    "        # Validar dimensiones\n",
    "        if y_batch.shape[1] != 5:\n",
    "            raise ValueError(\"Debe haber exactamente 5 etiquetas por muestra\")\n",
    "\n",
    "    def _update_stats(self, y_batch: np.ndarray) -> None:\n",
    "        \"\"\"Actualiza las estadísticas de conteo\"\"\"\n",
    "        for label, idx in zip(['direccion', 'fachada', 'envio', 'etiqueta', 'planilla'], range(5)):\n",
    "            self.class_stats[label]['original'] += y_batch[:, idx].sum()\n",
    "\n",
    "    def _needs_generation(self, label: str) -> bool:\n",
    "        \"\"\"Determina si una clase necesita más muestras\"\"\"\n",
    "        total = self.class_stats[label]['original'] + self.class_stats[label]['synthetic']\n",
    "        return total < self.target_samples\n",
    "\n",
    "    def _generate_safe_samples(self, X_class: np.ndarray, y_class: np.ndarray, \n",
    "                              label: str, pbar: tqdm) -> int:\n",
    "        \"\"\"Genera muestras sintéticas con validaciones\"\"\"\n",
    "        try:\n",
    "            if len(X_class) < self.k_neighbors + 1:\n",
    "                logger.warning(f\"Clase {label}: Muestras insuficientes ({len(X_class)}) para SMOTE\")\n",
    "                return 0\n",
    "\n",
    "            needed = self.target_samples - (self.class_stats[label]['original'] + self.class_stats[label]['synthetic'])\n",
    "            if needed <= 0:\n",
    "                return 0\n",
    "                \n",
    "            print(f'Generando para {label}')\n",
    "            knn = NearestNeighbors(n_neighbors=self.k_neighbors)\n",
    "            knn.fit(X_class.reshape(len(X_class), -1))\n",
    "            \n",
    "            generated = 0\n",
    "            for _ in range(min(needed, self.batch_size)):\n",
    "                i = np.random.randint(0, len(X_class))\n",
    "                neighbor_idx = np.random.choice(knn.kneighbors([X_class[i].flatten()])[1][0])\n",
    "                gap = np.random.uniform(0, 1)\n",
    "                \n",
    "                synthetic = np.clip(X_class[i] + gap * (X_class[neighbor_idx] - X_class[i]), 0, 1)\n",
    "                synth_hash = hashlib.md5(synthetic.tobytes()).hexdigest()\n",
    "                \n",
    "                if synth_hash not in self.existing_hashes:\n",
    "                    self._save_sample(synthetic, y_class[i], label, synth_hash)\n",
    "                    generated += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            return generated\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generando muestras para {label}: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def _save_sample(self, img_array: np.ndarray, y: np.ndarray, \n",
    "                    label: str, img_hash: str) -> None:\n",
    "        \"\"\"Guarda una muestra individual con registro robusto\"\"\"\n",
    "        try:\n",
    "            filename = f\"synth_{label}_{img_hash[:8]}.jpg\"\n",
    "            filepath = os.path.abspath(os.path.join(self.output_dir, filename))\n",
    "            \n",
    "            # Conversión validada a uint8\n",
    "            if img_array.dtype != np.uint8:\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "                \n",
    "            Image.fromarray(img_array).save(filepath)\n",
    "            \n",
    "            # Registrar en CSV\n",
    "            pd.DataFrame([{\n",
    "                'filename': filename,\n",
    "                'urlAbsoluta': filepath,\n",
    "                'direccion': int(y[0]),\n",
    "                'fachada': int(y[1]),\n",
    "                'envio': int(y[2]),\n",
    "                'etiqueta': int(y[3]),\n",
    "                'planilla': int(y[4])\n",
    "            }]).to_csv(self.csv_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            # Actualizar estado\n",
    "            self.existing_hashes.add(img_hash)\n",
    "            self.class_stats[label]['synthetic'] += 1\n",
    "            \n",
    "            # Registrar hash\n",
    "            with open(os.path.join(self.output_dir, 'image_hashes.txt'), 'a') as f:\n",
    "                f.write(f\"{img_hash}\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando muestra {filename}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _log_progress(self) -> None:\n",
    "        \"\"\"Registra el progreso actual\"\"\"\n",
    "        progress = []\n",
    "        for label in self.class_stats:\n",
    "            total = self.class_stats[label]['original'] + self.class_stats[label]['synthetic']\n",
    "            progress.append(\n",
    "                f\"{label}: {total}/{self.target_samples} \"\n",
    "                f\"({min(100, total/self.target_samples*100):.1f}%)\"\n",
    "            )\n",
    "        logger.info(\"Progreso | \" + \" | \".join(progress))\n",
    "\n",
    "    def fit_resample(self, data_loader: Generator[Tuple[np.ndarray, np.ndarray], None, None]) -> None:\n",
    "        \"\"\"Ejecuta el proceso completo con seguimiento detallado\"\"\"\n",
    "        total_batches = len(data_loader) if hasattr(data_loader, '__len__') else None\n",
    "        progress_desc = \"Procesando dataset \" + (f\" ({total_batches} lotes)\" if total_batches else \" \")\n",
    "        \n",
    "        try:\n",
    "            with tqdm(data_loader, desc=progress_desc, unit=\"batch\", total=total_batches) as batch_pbar:\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(batch_pbar):\n",
    "                    # Validar lote\n",
    "                    self._validate_batch(X_batch, y_batch)\n",
    "                    \n",
    "                    # Actualizar estadísticas\n",
    "                    self._update_stats(y_batch)\n",
    "                    \n",
    "                    # Procesar cada clase\n",
    "                    with tqdm(total=5, desc=\"Clases\", leave=False) as class_pbar:\n",
    "                        for label in ['direccion', 'fachada', 'envio', 'etiqueta', 'planilla']:\n",
    "                            if self._needs_generation(label):\n",
    "                                mask = y_batch[:, list(self.class_stats.keys()).index(label)] == 1\n",
    "                                X_class = X_batch[mask]\n",
    "                                y_class = y_batch[mask]\n",
    "                                \n",
    "                                generated = self._generate_safe_samples(X_class, y_class, label, batch_pbar)\n",
    "                                if generated > 0:\n",
    "                                    logger.debug(f\"Lote {batch_idx}: Generadas {generated} para {label}\")\n",
    "                                    \n",
    "                            class_pbar.update(1)\n",
    "                            class_pbar.refresh()\n",
    "                    \n",
    "                    # Liberar memoria\n",
    "                    del X_batch, y_batch\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Reporte periódico\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        self._log_progress()\n",
    "                        \n",
    "            # Reporte final\n",
    "            logger.info(\"\\nPROCESO COMPLETADO\")\n",
    "            self._log_progress()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en el proceso principal: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Cierre seguro de recursos\n",
    "            if 'f' in locals():\n",
    "                f.close()\n",
    "            logger.info(\"Limpieza finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ed47daa-3bf5-4e9f-bce5-029c7ed8f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelSMOTE...\n",
      "Distribución antes de SMOTE\n",
      "Dataset preparado con 4231 imágenes\n",
      "Distribución de clases:\n",
      "direccion: 16.85% (713/4231)\n",
      "fachada: 18.86% (798/4231)\n",
      "envio: 53.27% (2254/4231)\n",
      "etiqueta: 38.17% (1615/4231)\n",
      "planilla: 17.54% (742/4231)\n",
      "Frecuencia de cada etiqueta: [713, 798, 2254, 1615, 742]\n",
      "La etiqueta que más aparece es la 2 con 2254 apariciones\n",
      "Umbral de generación: 2141\n",
      "Generando data sintética...\n",
      "{'direccion': True, 'fachada': True, 'envio': False, 'etiqueta': True, 'planilla': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 0batch [00:00, ?batch/s]\n",
      "Procesando dataset  : 4batch [00:07,  1.52s/batch]                                               | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para direccion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 457batch [00:25, 22.69batch/s]\n",
      "\u001b[Ases:  20%|███████████████▏                                                            | 1/5 [00:17<01:11, 17.84s/it]\n",
      "Procesando dataset  : 460batch [00:25, 19.15batch/s]                                     | 1/5 [00:17<01:11, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para fachada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 881batch [00:45, 21.88batch/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:37<00:56, 18.89s/it]\n",
      "Procesando dataset  : 884batch [00:45, 14.69batch/s]                                     | 2/5 [00:37<00:56, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para envio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 1365batch [01:15, 15.67batch/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████▌                              | 3/5 [01:07<00:47, 23.93s/it]\n",
      "Procesando dataset  : 1367batch [01:15, 11.77batch/s]█████▌                              | 3/5 [01:07<00:47, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para etiqueta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 1816batch [01:39, 19.23batch/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████▊               | 4/5 [01:32<00:24, 24.26s/it]\n",
      "Procesando dataset  : 1822batch [01:40, 19.82batch/s]████████████████████▊               | 4/5 [01:32<00:24, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para planilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 2240batch [01:56, 27.48batch/s]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:49<00:00, 21.59s/it]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:49<00:00, 21.59s/it]\n",
      "\u001b[A2025-03-08 04:15:03,953 - INFO - Progreso | direccion: 528/2141 (24.7%) | fachada: 522/2141 (24.4%) | envio: 748/2141 (34.9%) | etiqueta: 643/2141 (30.0%) | planilla: 479/2141 (22.4%)\n",
      "\n",
      "Procesando dataset  : 2243batch [02:02,  1.60batch/s]                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para direccion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 2693batch [02:20, 22.45batch/s]\n",
      "\u001b[Ases:  20%|███████████████▏                                                            | 1/5 [00:18<01:13, 18.44s/it]\n",
      "Procesando dataset  : 2699batch [02:21, 22.10batch/s]                                    | 1/5 [00:18<01:13, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para fachada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 3119batch [02:39, 23.23batch/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:37<00:56, 18.71s/it]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:37<00:56, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para envio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 3604batch [03:12, 13.91batch/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████▌                              | 3/5 [01:09<00:50, 25.04s/it]\n",
      "Procesando dataset  : 3606batch [03:12, 10.51batch/s]█████▌                              | 3/5 [01:09<00:50, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para etiqueta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 4064batch [03:42, 15.90batch/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████▊               | 4/5 [01:40<00:27, 27.17s/it]\n",
      "Procesando dataset  : 4069batch [03:43, 18.74batch/s]████████████████████▊               | 4/5 [01:40<00:27, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para planilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 4506batch [04:00, 24.35batch/s]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:58<00:00, 23.89s/it]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:58<00:00, 23.89s/it]\n",
      "\u001b[A                                                                                                                    \n",
      "Procesando dataset  : 4509batch [04:07,  1.53batch/s]                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para direccion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 4972batch [04:29, 20.20batch/s]\n",
      "\u001b[Ases:  20%|███████████████▏                                                            | 1/5 [00:22<01:29, 22.40s/it]\n",
      "Procesando dataset  : 4975batch [04:29, 17.18batch/s]                                    | 1/5 [00:22<01:29, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para fachada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 5403batch [04:52, 16.98batch/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:45<01:07, 22.52s/it]\n",
      "Procesando dataset  : 5405batch [04:52, 13.44batch/s]                                    | 2/5 [00:45<01:07, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para envio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 5785batch [05:13, 19.24batch/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████▌                              | 3/5 [01:05<00:43, 21.81s/it]\n",
      "Procesando dataset  : 5787batch [05:13, 13.32batch/s]█████▌                              | 3/5 [01:05<00:43, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para etiqueta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 6227batch [05:36, 19.00batch/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████▊               | 4/5 [01:29<00:22, 22.35s/it]\n",
      "Procesando dataset  : 6230batch [05:36, 16.43batch/s]████████████████████▊               | 4/5 [01:29<00:22, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para planilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 6703batch [05:59, 20.33batch/s]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:52<00:00, 22.65s/it]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [01:52<00:00, 22.65s/it]\n",
      "Procesando dataset  : 6707batch [05:59, 21.54batch/s]                                                                  \n",
      "Procesando dataset  : 6710batch [06:05,  1.68batch/s]                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para direccion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 7085batch [06:21, 19.01batch/s]\n",
      "\u001b[Ases:  20%|███████████████▏                                                            | 1/5 [00:16<01:06, 16.50s/it]\n",
      "Procesando dataset  : 7087batch [06:21, 16.99batch/s]                                    | 1/5 [00:16<01:06, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para fachada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 7476batch [06:40, 22.45batch/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:35<00:53, 17.83s/it]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:35<00:53, 17.83s/it]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████▌                              | 3/5 [00:35<00:35, 17.83s/it]\n",
      "Procesando dataset  : 7479batch [06:40, 18.57batch/s]████████████████████▊               | 4/5 [00:35<00:17, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para planilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 7901batch [07:00, 21.36batch/s]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:55<00:00,  9.83s/it]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:55<00:00,  9.83s/it]\n",
      "\u001b[A                                                                                                                    \n",
      "\u001b[Ases:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "Procesando dataset  : 7904batch [07:06,  1.64batch/s]                                            | 1/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para fachada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset  : 7913batch [07:06,  4.31batch/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:00<00:00,  3.81it/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████▍                                             | 2/5 [00:00<00:00,  3.81it/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████▌                              | 3/5 [00:00<00:00,  3.81it/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████▊               | 4/5 [00:00<00:00,  3.81it/s]\n",
      "\u001b[Ases: 100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  3.81it/s]\n",
      "\u001b[A                                                                                                                    \n",
      "\u001b[Ases:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  20%|████████████████▊                                                                   | 1/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████                                             | 2/5 [00:00<00:00, 185.48it/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████                              | 3/5 [00:00<00:00, 278.22it/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████               | 4/5 [00:00<00:00, 370.96it/s]\n",
      "\u001b[Ases: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 463.69it/s]\n",
      "\u001b[A                                                                                                                    \n",
      "\u001b[Ases:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  20%|███████████████                                                            | 1/5 [00:00<00:00, 649.37it/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████                                             | 2/5 [00:00<00:00, 461.72it/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████                              | 3/5 [00:00<00:00, 187.20it/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████               | 4/5 [00:00<00:00, 178.67it/s]\n",
      "\u001b[Ases: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 155.19it/s]\n",
      "Procesando dataset  : 7914batch [07:23,  4.31batch/s]                                                                  \n",
      "\u001b[Ases:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  20%|███████████████▏                                                            | 1/5 [00:00<00:00, 94.18it/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████                                             | 2/5 [00:00<00:00, 188.36it/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████                              | 3/5 [00:00<00:00, 191.12it/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████               | 4/5 [00:00<00:00, 254.83it/s]\n",
      "\u001b[Ases: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 162.87it/s]\n",
      "\u001b[A                                                                                                                    \n",
      "\u001b[Ases:   0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  20%|████████████████▊                                                                   | 1/5 [00:00<?, ?it/s]\n",
      "\u001b[Ases:  40%|██████████████████████████████                                             | 2/5 [00:00<00:00, 429.81it/s]\n",
      "\u001b[Ases:  60%|█████████████████████████████████████████████                              | 3/5 [00:00<00:00, 644.72it/s]\n",
      "\u001b[Ases:  80%|████████████████████████████████████████████████████████████               | 4/5 [00:00<00:00, 859.62it/s]\n",
      "\u001b[Ases: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 276.04it/s]\n",
      "Procesando dataset  : 9batch [07:27, 49.68s/batch]                                                                     \n",
      "2025-03-08 04:20:34,090 - INFO - \n",
      "PROCESO COMPLETADO\n",
      "2025-03-08 04:20:34,104 - INFO - Progreso | direccion: 2470/2141 (100.0%) | fachada: 2482/2141 (100.0%) | envio: 3604/2141 (100.0%) | etiqueta: 2970/2141 (100.0%) | planilla: 2507/2141 (100.0%)\n",
      "2025-03-08 04:20:34,106 - INFO - Limpieza finalizada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de data sintética generada con SMOTE\n",
      "Dataset preparado con 7911 imágenes\n",
      "Distribución de clases:\n",
      "direccion: 36.35% (2876/7911)\n",
      "fachada: 39.51% (3126/7911)\n",
      "envio: 32.68% (2585/7911)\n",
      "etiqueta: 28.54% (2258/7911)\n",
      "planilla: 22.48% (1778/7911)\n",
      "MultiLabelSMOTE OK\n"
     ]
    }
   ],
   "source": [
    "# Aplicar SMOTE adaptado\n",
    "print('MultiLabelSMOTE...')\n",
    "# calcular la mínima cantidad de muestrar a generar con un grado de tolerancia\n",
    "# Suma por columna para obtener la frecuencia de cada etiqueta\n",
    " # Leer solo las columnas necesarias del CSV\n",
    "print('Distribución antes de SMOTE')\n",
    "total_samples, frecuencias = print_class_distribution_from_csv(CSV_TRAIN, LABEL_COLUMNS)\n",
    "# Obtener el valor máximo (la cantidad máxima de veces que aparece una etiqueta)\n",
    "max_frecuencia = np.max(frecuencias)\n",
    "# Ver cuál etiqueta es la que más aparece\n",
    "etiqueta_mas_comun = np.argmax(frecuencias) \n",
    "\n",
    "print(f'Frecuencia de cada etiqueta: {frecuencias}')\n",
    "print(f'La etiqueta que más aparece es la {etiqueta_mas_comun} con {max_frecuencia} apariciones')\n",
    "max_frecuencia = int(max_frecuencia - (max_frecuencia * 0.05))\n",
    "print(f'Umbral de generación: {max_frecuencia}')\n",
    "\n",
    "print('Generando data sintética...')\n",
    "# Configurar con batch_size pequeño para baja memoria\n",
    "mlsmote = MultiLabelSMOTE(\n",
    "    target_samples=max_frecuencia,\n",
    "    output_dir='./synthetic_data',\n",
    "    batch_size=500  # Ajustar según memoria disponible\n",
    ")\n",
    "\n",
    "mlsmote.needs_smote = {\n",
    "            'direccion': frecuencias[0] < max_frecuencia,\n",
    "            'fachada': frecuencias[1] < max_frecuencia,\n",
    "            'envio': frecuencias[2] < max_frecuencia,\n",
    "            'etiqueta': frecuencias[3] < max_frecuencia,\n",
    "            'planilla': frecuencias[4] < max_frecuencia,\n",
    "        }\n",
    "\n",
    "mlsmote.original_counts = {\n",
    "            'direccion': frecuencias[0],\n",
    "            'fachada': frecuencias[1],\n",
    "            'envio': frecuencias[2],\n",
    "            'etiqueta': frecuencias[3],\n",
    "            'planilla': frecuencias[4]\n",
    "        }\n",
    "\n",
    "print(mlsmote.needs_smote)\n",
    "\n",
    "# Ejecución\n",
    "mlsmote.fit_resample(batch_loader(csv_path=CSV_TRAIN, local_image_path=LOCAL_IMAGE_PATH, label_columns=LABEL_COLUMNS, target_size=TARGET_SIZE, batch_size=500))\n",
    "\n",
    "print('Distribución de data sintética generada con SMOTE')\n",
    "print_class_distribution_from_csv('./synthetic_data/metadata.csv', label_columns=LABEL_COLUMNS)\n",
    "print('MultiLabelSMOTE OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c70b7d0-3087-42f5-a861-358e7df23523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras para direccion: 2876 (Objetivo: 2141)\n",
      "Muestras para fachada: 3126 (Objetivo: 2141)\n",
      "Muestras para envio: 2585 (Objetivo: 2141)\n",
      "Muestras para etiqueta: 2258 (Objetivo: 2141)\n",
      "Muestras para planilla: 1778 (Objetivo: 2141)\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv('./synthetic_data/metadata.csv')\n",
    "for label in LABEL_COLUMNS:\n",
    "    count = df_final[label].sum()\n",
    "    print(f\"Muestras para {label}: {count} (Objetivo: {mlsmote.target_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d9d9b-7e89-48e9-bd0f-c49672e45a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
