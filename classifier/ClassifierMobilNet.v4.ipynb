{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3065-6f41-4438-9400-8f497bf55fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import Sequence\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Silenciar mensajes de INFO y WARNING\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35105385-aa7e-4a73-b195-3d277db47c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveDataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, target_size, augment=False, **kwargs):\n",
    "        super().__init__(**kwargs)  # Llamar al constructor de la clase base\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # Capas de aumento de datos de Keras\n",
    "        self.data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            tf.keras.layers.RandomRotation(0.2),\n",
    "            tf.keras.layers.RandomZoom(0.2),\n",
    "            tf.keras.layers.RandomContrast(0.2),\n",
    "        ])\n",
    "\n",
    "        # Calcular el desbalanceo de clases\n",
    "        self.class_counts = Counter(np.argmax(self.df[['direccion', 'fachada', 'envio', 'etiqueta']].values, axis=1))\n",
    "        self.max_class_count = max(self.class_counts.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        images = []\n",
    "        labels = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            image = self.load_image(row)\n",
    "            label = row[['direccion', 'fachada', 'envio', 'etiqueta']].values.astype(np.float32)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Aplicar data augmentation adaptativa\n",
    "        if self.augment:\n",
    "            augmented_images = []\n",
    "            augmented_labels = []\n",
    "            for i in range(len(images)):\n",
    "                class_idx = np.argmax(labels[i])\n",
    "                if self.class_counts[class_idx] < self.max_class_count:\n",
    "                    # Aplicar data augmentation a las clases minoritarias\n",
    "                    augmented_image = self.data_augmentation(images[i], training=True)\n",
    "                    augmented_images.append(augmented_image)\n",
    "                    augmented_labels.append(labels[i])\n",
    "            if augmented_images:\n",
    "                images = np.concatenate([images, np.array(augmented_images)], axis=0)\n",
    "                labels = np.concatenate([labels, np.array(augmented_labels)], axis=0)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def load_image(self, row):\n",
    "        if pd.notna(row['filename']):\n",
    "            img_path = os.path.join(\n",
    "                \"/home/tulua/Documents/Projects/ClassifierImage/repo_dataset/\", row['filename']\n",
    "            )\n",
    "            # Cargar imagen desde archivo local usando Pillow\n",
    "            image = Image.open(img_path)\n",
    "        elif pd.notna(row['urlAbsoluta']):\n",
    "            # Descargar imagen desde URL\n",
    "            response = requests.get(row['urlAbsoluta'])\n",
    "            if response.status_code == 200:\n",
    "                image = Image.open(BytesIO(response.content))\n",
    "            else:\n",
    "                raise ValueError(f\"No se pudo descargar la imagen desde {row['urlAbsoluta']}\")\n",
    "        else:\n",
    "            raise ValueError(\"No se pudo cargar la imagen.\")\n",
    "\n",
    "        # Convertir a RGB (en caso de que la imagen esté en otro formato, como RGBA)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        # Redimensionar la imagen\n",
    "        image = image.resize((224, 224))  # Redimensionar a 224x224 para MobileNetV3\n",
    "        # Convertir a un array de numpy y normalizar\n",
    "        image = np.array(image) / 255.0  # Normalizar\n",
    "        return image\n",
    "\n",
    "    def get_all_labels(self):\n",
    "        \"\"\"\n",
    "        Obtiene todas las etiquetas después de aplicar SMOTE y data augmentation.\n",
    "        \"\"\"\n",
    "        all_labels = []\n",
    "        for i in range(len(self)):\n",
    "            _, labels = self[i]\n",
    "            all_labels.extend(labels)\n",
    "        return np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbebe9-36db-48e3-adcf-748cbb84fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(row):\n",
    "    if pd.notna(row['filename']):\n",
    "        # Cargar imagen desde archivo local\n",
    "        img_path = os.path.join(\n",
    "                \"/home/tulua/Documents/Projects/ClassifierImage/repo_dataset/\", row['filename']\n",
    "            )\n",
    "        # Cargar imagen desde archivo local usando Pillow\n",
    "        image = Image.open(img_path)\n",
    "    elif pd.notna(row['urlAbsoluta']):\n",
    "        # Descargar imagen desde URL\n",
    "        response = requests.get(row['urlAbsoluta'])\n",
    "        if response.status_code == 200:\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "        else:\n",
    "            raise ValueError(f\"No se pudo descargar la imagen desde {row['urlAbsoluta']}\")\n",
    "    else:\n",
    "        raise ValueError(f\"No se pudo cargar la imagen. {row['filename']}\")\n",
    "    \n",
    "        # Convertir a RGB (en caso de que la imagen esté en otro formato, como RGBA)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "    # Redimensionar la imagen\n",
    "    image = image.resize((224, 224))  # Redimensionar a 224x224 para MobileNetV3\n",
    "    # Convertir a un array de numpy y normalizar\n",
    "    image = np.array(image) / 255.0  # Normalizar\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bba4e-9f8a-40d5-9879-d654dcc2bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_porcentaje_clases(df, clases):\n",
    "    \"\"\"\n",
    "    Calcula el porcentaje de cada clase en el DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las etiquetas.\n",
    "        clases (list): Lista de nombres de las columnas de las clases.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con el porcentaje de cada clase.\n",
    "    \"\"\"\n",
    "    # Calcular el número de muestras por clase\n",
    "    conteo_clases = df[clases].sum().to_dict()\n",
    "    \n",
    "    # Calcular el porcentaje de cada clase\n",
    "    total_muestras = len(df)\n",
    "    porcentaje_clases = {clase: (conteo / total_muestras) * 100 for clase, conteo in conteo_clases.items()}\n",
    "    \n",
    "    return porcentaje_clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b3977-bf04-4a1e-b311-c32a94cfce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el porcentaje de cada clase\n",
    "def calcular_porcentaje_clases_augmented(labels, clases):\n",
    "    \"\"\"\n",
    "    Calcula el porcentaje de cada clase en las etiquetas.\n",
    "    \n",
    "    Args:\n",
    "        labels (np.array): Etiquetas después de SMOTE y data augmentation.\n",
    "        clases (list): Lista de nombres de las columnas de las clases.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con el porcentaje de cada clase.\n",
    "    \"\"\"\n",
    "    conteo_clases = np.sum(labels, axis=0)\n",
    "    total_muestras = len(labels)\n",
    "    porcentaje_clases = {clase: (conteo / total_muestras) * 100 for clase, conteo in zip(clases, conteo_clases)}\n",
    "    return porcentaje_clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bca45-2930-478c-b09f-0c21d7f65cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('./mobilnet-multi-label.csv')\n",
    "\n",
    "# Seleccionar solo el 40% de los datos\n",
    "df_sample = df.sample(frac=0.4, random_state=42)\n",
    "\n",
    "# Dividir el subconjunto en 50% para entrenamiento y 50% para validación\n",
    "train_df, val_df = train_test_split(df_sample, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_df)}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_df)}\")\n",
    "\n",
    "# Calcular el porcentaje de cada clase en el conjunto de entrenamiento\n",
    "clases = ['direccion', 'fachada', 'envio', 'etiqueta']\n",
    "porcentaje_clases = calcular_porcentaje_clases(train_df, clases)\n",
    "\n",
    "# Mostrar el porcentaje de cada clase\n",
    "print(\"Porcentaje de cada clase en el conjunto de entrenamiento:\")\n",
    "for clase, porcentaje in porcentaje_clases.items():\n",
    "    print(f\"{clase}: {porcentaje:.2f}%\")\n",
    "\n",
    "# Crear un generador de datos para aplicar SMOTE\n",
    "def smote_data_generator(df, batch_size, target_size):\n",
    "    while True:\n",
    "        # Cargar un lote de imágenes y etiquetas\n",
    "        batch_df = df.sample(n=batch_size)\n",
    "        images = np.array([load_image(row) for _, row in batch_df.iterrows()])\n",
    "        labels = batch_df[['direccion', 'fachada', 'envio', 'etiqueta']].values\n",
    "        \n",
    "        # Aplanar las imágenes para SMOTE\n",
    "        images_flattened = images.reshape(images.shape[0], -1)\n",
    "        \n",
    "        # Aplicar SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        images_resampled, labels_resampled = smote.fit_resample(images_flattened, labels)\n",
    "        \n",
    "        # Volver a la forma original de las imágenes\n",
    "        images_resampled = images_resampled.reshape(-1, *target_size, 3)\n",
    "        \n",
    "        yield images_resampled, labels_resampled\n",
    "\n",
    "# Crear generadores de datos\n",
    "batch_size = 8\n",
    "target_size = (224, 224)\n",
    "train_generator = AdaptiveDataGenerator(\n",
    "    train_df, \n",
    "    batch_size, \n",
    "    target_size, \n",
    "    augment=True, \n",
    "    workers=4,  # Pasar aquí los parámetros de optimización\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=10)\n",
    "val_generator = AdaptiveDataGenerator(\n",
    "    val_df, \n",
    "    batch_size, \n",
    "    target_size, \n",
    "    augment=False, \n",
    "    workers=4,  # Pasar aquí los parámetros de optimización\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=10)\n",
    "\n",
    "# Obtener todas las etiquetas después de SMOTE y data augmentation\n",
    "all_labels = train_generator.get_all_labels()\n",
    "\n",
    "# Calcular el porcentaje de cada clase\n",
    "clases = ['direccion', 'fachada', 'envio', 'etiqueta']\n",
    "porcentaje_clases = calcular_porcentaje_clases_augmented(all_labels, clases)\n",
    "\n",
    "# Mostrar el porcentaje de cada clase\n",
    "print(\"Porcentaje de cada clase después de SMOTE y data augmentation:\")\n",
    "for clase, porcentaje in porcentaje_clases.items():\n",
    "    print(f\"{clase}: {porcentaje:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a370606-078c-4dff-ae47-8a23d9150578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV3Large\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Cargar MobileNetV3 con pesos preentrenados en ImageNet\n",
    "base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelar las capas del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir capas personalizadas para la clasificación multi-label\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(4, activation='sigmoid')(x)  # 4 etiquetas: direccion, fachada, envio, etiqueta\n",
    "\n",
    "# Crear el modelo final\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ee700-491d-4bbf-8d61-bf6b12c6c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595937d5-45e2-42b8-9a1e-f60d591032b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de validación\n",
    "loss, accuracy = model.evaluate(val_generator, verbose=1)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442644db-365e-48ac-a425-095d1423396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar la pérdida y la precisión durante el entrenamiento\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Pérdida durante el entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Precisión durante el entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42787e9-3ed1-4efe-8d06-29334dda9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save(\"mobilenetv3_classifier.v.4.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
